From 98a73a2f4ad09c4d305edff39cd2e4067bbc5c3e Mon Sep 17 00:00:00 2001
From: Geoff Phillips <geoff@unumotors.com>
Date: Mon, 18 May 2020 17:40:42 +0200
Subject: [PATCH] Expose SDMA API

Patches based on meta-openglow patches:
https://github.com/ScottW514/meta-openglow
Additional interfaces:
- sdma_set_channel_pending_by_mask
- sdma_event_enable_by_channel_mask
- sdma_event_disable_by_channel_mask
- sdma_is_event_enabled
Added spinlock locking to and return the old value to:
- sdma_event_enable
- sdma_event_disable
---
 drivers/dma/imx-sdma.c                     | 581 +++++++++++++--------
 include/linux/platform_data/dma-imx-sdma.h | 328 +++++++++++-
 2 files changed, 697 insertions(+), 212 deletions(-)

diff --git a/drivers/dma/imx-sdma.c b/drivers/dma/imx-sdma.c
index bc3922fd7fac..9adf3cb5abc9 100644
--- a/drivers/dma/imx-sdma.c
+++ b/drivers/dma/imx-sdma.c
@@ -34,6 +34,7 @@
 #include <linux/genalloc.h>
 #include <linux/dma-mapping.h>
 #include <linux/dmapool.h>
+#include <linux/dmapool.h>
 #include <linux/firmware.h>
 #include <linux/slab.h>
 #include <linux/platform_device.h>
@@ -184,129 +185,6 @@
 #define SDMA_WATERMARK_LEVEL_HWE	BIT(29)
 #define SDMA_WATERMARK_LEVEL_CONT	BIT(31)
 
-#define SDMA_DMA_BUSWIDTHS	(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_3_BYTES) | \
-				 BIT(DMA_SLAVE_BUSWIDTH_4_BYTES))
-
-#define SDMA_DMA_DIRECTIONS	(BIT(DMA_DEV_TO_MEM) | \
-				 BIT(DMA_MEM_TO_DEV) | \
-				 BIT(DMA_DEV_TO_DEV))
-
-#define SDMA_WATERMARK_LEVEL_FIFOS_OFF	8
-#define SDMA_WATERMARK_LEVEL_SW_DONE	BIT(23)
-#define SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF 24
-
-/*
- * Mode/Count of data node descriptors - IPCv2
- */
-struct sdma_mode_count {
-	u32 count   : 16; /* size of the buffer pointed by this BD */
-	u32 status  :  8; /* E,R,I,C,W,D status bits stored here */
-	u32 command :  8; /* command mostly used for channel 0 */
-};
-
-/*
- * Buffer descriptor
- */
-struct sdma_buffer_descriptor {
-	struct sdma_mode_count  mode;
-	u32 buffer_addr;	/* address of the buffer described */
-	u32 ext_buffer_addr;	/* extended buffer address */
-} __attribute__ ((packed));
-
-/**
- * struct sdma_channel_control - Channel control Block
- *
- * @current_bd_ptr	current buffer descriptor processed
- * @base_bd_ptr		first element of buffer descriptor array
- * @unused		padding. The SDMA engine expects an array of 128 byte
- *			control blocks
- */
-struct sdma_channel_control {
-	u32 current_bd_ptr;
-	u32 base_bd_ptr;
-	u32 unused[2];
-} __attribute__ ((packed));
-
-/**
- * struct sdma_state_registers - SDMA context for a channel
- *
- * @pc:		program counter
- * @t:		test bit: status of arithmetic & test instruction
- * @rpc:	return program counter
- * @sf:		source fault while loading data
- * @spc:	loop start program counter
- * @df:		destination fault while storing data
- * @epc:	loop end program counter
- * @lm:		loop mode
- */
-struct sdma_state_registers {
-	u32 pc     :14;
-	u32 unused1: 1;
-	u32 t      : 1;
-	u32 rpc    :14;
-	u32 unused0: 1;
-	u32 sf     : 1;
-	u32 spc    :14;
-	u32 unused2: 1;
-	u32 df     : 1;
-	u32 epc    :14;
-	u32 lm     : 2;
-} __attribute__ ((packed));
-
-/**
- * struct sdma_context_data - sdma context specific to a channel
- *
- * @channel_state:	channel state bits
- * @gReg:		general registers
- * @mda:		burst dma destination address register
- * @msa:		burst dma source address register
- * @ms:			burst dma status register
- * @md:			burst dma data register
- * @pda:		peripheral dma destination address register
- * @psa:		peripheral dma source address register
- * @ps:			peripheral dma status register
- * @pd:			peripheral dma data register
- * @ca:			CRC polynomial register
- * @cs:			CRC accumulator register
- * @dda:		dedicated core destination address register
- * @dsa:		dedicated core source address register
- * @ds:			dedicated core status register
- * @dd:			dedicated core data register
- */
-struct sdma_context_data {
-	struct sdma_state_registers  channel_state;
-	u32  gReg[8];
-	u32  mda;
-	u32  msa;
-	u32  ms;
-	u32  md;
-	u32  pda;
-	u32  psa;
-	u32  ps;
-	u32  pd;
-	u32  ca;
-	u32  cs;
-	u32  dda;
-	u32  dsa;
-	u32  ds;
-	u32  dd;
-	u32  scratch0;
-	u32  scratch1;
-	u32  scratch2;
-	u32  scratch3;
-	u32  scratch4;
-	u32  scratch5;
-	u32  scratch6;
-	u32  scratch7;
-} __attribute__ ((packed));
-
-#define NUM_BD (int)(PAGE_SIZE / sizeof(struct sdma_buffer_descriptor))
-#define SDMA_BD_MAX_CNT	0xfffc /* align with 4 bytes */
-
-struct sdma_engine;
-
 struct sdma_desc {
 	struct virt_dma_desc		vd;
 	struct list_head		node;
@@ -362,11 +240,13 @@ struct sdma_channel {
 	u32				bd_size_sum;
 	bool				src_dualfifo;
 	bool				dst_dualfifo;
-	unsigned int			fifo_num;
 	struct dma_pool			*bd_pool;
+	struct dma_async_tx_descriptor 	cb;
+	struct tasklet_struct		cb_task;
 };
 
 #define IMX_DMA_SG_LOOP		BIT(0)
+#define IMX_DMA_CUSTOM_CALLBACK	BIT(1)
 
 #define MAX_DMA_CHANNELS 32
 #define MXC_SDMA_DEFAULT_PRIORITY 1
@@ -434,16 +314,25 @@ struct sdma_engine {
 	u32				spba_end_addr;
 	unsigned int			irq;
 	struct gen_pool 		*iram_pool;
+	void				*tiny_datamem_buf;
+	dma_addr_t			tiny_datamem_buf_phys;
 	/* channel0 bd */
 	dma_addr_t			bd0_phys;
 	bool				bd0_iram;
 	struct sdma_buffer_descriptor	*bd0;
 	bool				suspend_off;
-	int				idx;
-	/* clock ration for AHB:SDMA core. 1:1 is 1, 2:1 is 0*/
-	bool				clk_ratio;
+	spinlock_t			event_enable_lock;
 };
 
+/**
+ * If nonzero, sdma_write_datamem()/sdma_fetch_datamem()
+ * use a single preallocated temporary buffer for transfers whose size is
+ * less than or equal to this value.
+ */
+#define TINY_DATAMEM_BUF_SIZE	PAGE_SIZE
+
+static struct sdma_engine *sdma_singleton = NULL;
+
 static struct sdma_driver_data sdma_imx31 = {
 	.chnenbl0 = SDMA_CHNENBL0_IMX31,
 	.num_events = 32,
@@ -575,12 +464,6 @@ static struct sdma_driver_data sdma_imx7d = {
 	.script_addrs = &sdma_script_imx7d,
 };
 
-static struct sdma_driver_data sdma_imx8m = {
-	.chnenbl0 = SDMA_CHNENBL0_IMX35,
-	.num_events = 48,
-	.script_addrs = &sdma_script_imx7d,
-};
-
 static const struct platform_device_id sdma_devtypes[] = {
 	{
 		.name = "imx25-sdma",
@@ -606,9 +489,6 @@ static const struct platform_device_id sdma_devtypes[] = {
 	}, {
 		.name = "imx7d-sdma",
 		.driver_data = (unsigned long)&sdma_imx7d,
-	}, {
-		.name = "imx8mq-sdma",
-		.driver_data = (unsigned long)&sdma_imx8m,
 	}, {
 		/* sentinel */
 	}
@@ -625,18 +505,22 @@ static const struct of_device_id sdma_dt_ids[] = {
 	{ .compatible = "fsl,imx31-sdma", .data = &sdma_imx31, },
 	{ .compatible = "fsl,imx25-sdma", .data = &sdma_imx25, },
 	{ .compatible = "fsl,imx7d-sdma", .data = &sdma_imx7d, },
-	{ .compatible = "fsl,imx8mq-sdma", .data = &sdma_imx8m, },
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, sdma_dt_ids);
 
-static int sdma_dev_idx;
-
 #define SDMA_H_CONFIG_DSPDMA	BIT(12) /* indicates if the DSPDMA is used */
 #define SDMA_H_CONFIG_RTD_PINS	BIT(11) /* indicates if Real-Time Debug pins are enabled */
 #define SDMA_H_CONFIG_ACR	BIT(4)  /* indicates if AHB freq /core freq = 2 or 1 */
 #define SDMA_H_CONFIG_CSM	(3)       /* indicates which context switch mode is selected*/
 
+/* returns an address in data space (32-bit words) */
+u32 sdma_channel_context_base(int ch)
+{
+	return 2048 + (sizeof(struct sdma_context_data) / 4) * ch;
+}
+EXPORT_SYMBOL(sdma_channel_context_base);
+
 static void sdma_start_desc(struct sdma_channel *sdmac);
 
 static inline u32 chnenbl_ofs(struct sdma_engine *sdma, unsigned int event)
@@ -645,7 +529,7 @@ static inline u32 chnenbl_ofs(struct sdma_engine *sdma, unsigned int event)
 	return chnenbl0 + event * 4;
 }
 
-static int sdma_config_ownership(struct sdma_channel *sdmac,
+int sdma_config_ownership(struct sdma_channel *sdmac,
 		bool event_override, bool mcu_override, bool dsp_override)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
@@ -680,6 +564,7 @@ static int sdma_config_ownership(struct sdma_channel *sdmac,
 
 	return 0;
 }
+EXPORT_SYMBOL(sdma_config_ownership);
 
 static void sdma_enable_channel(struct sdma_engine *sdma, int channel)
 {
@@ -708,7 +593,7 @@ static int sdma_run_channel0(struct sdma_engine *sdma)
 	return ret;
 }
 
-static int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
+int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
 		u32 address)
 {
 	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
@@ -747,30 +632,186 @@ static int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
 
 	return ret;
 }
+EXPORT_SYMBOL(sdma_load_script);
+
+int sdma_write_datamem(struct sdma_engine *sdma, void *buf, int size,
+		u32 address)
+{
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	void *buf_virt;
+	dma_addr_t buf_phys;
+	int ret;
+	unsigned long flags;
+	bool use_iram = true;
+	bool tiny = (size <= TINY_DATAMEM_BUF_SIZE);
+
+	if (!tiny) {
+		buf_virt = gen_pool_dma_alloc(sdma->iram_pool, size, &buf_phys);
+		if (!buf_virt) {
+			use_iram = false;
+			buf_virt = dma_alloc_coherent(NULL, size, &buf_phys,
+					GFP_KERNEL);
+			if (!buf_virt)
+				return -ENOMEM;
+		}
+	} else {
+		buf_virt = sdma->tiny_datamem_buf;
+		buf_phys = sdma->tiny_datamem_buf_phys;
+	}
+
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	bd0->mode.command = C0_SETDM;
+	bd0->mode.status = BD_DONE | BD_INTR | BD_WRAP | BD_EXTD;
+	bd0->mode.count = size / 4;
+	bd0->buffer_addr = buf_phys;
+	bd0->ext_buffer_addr = address;
+
+	memcpy(buf_virt, buf, size);
+
+	ret = sdma_run_channel0(sdma);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	if (!tiny) {
+		if (use_iram)
+			gen_pool_free(sdma->iram_pool, (unsigned long)buf_virt,
+					size);
+		else
+			dma_free_coherent(NULL, size, buf_virt, buf_phys);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(sdma_write_datamem);
+
+int sdma_fetch_datamem(struct sdma_engine *sdma, void *buf, int size,
+		u32 address)
+{
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	void *buf_virt;
+	dma_addr_t buf_phys;
+	int ret;
+	unsigned long flags;
+	bool use_iram = true;
+	bool tiny = (size <= TINY_DATAMEM_BUF_SIZE);
+
+	if (!tiny) {
+		buf_virt = gen_pool_dma_alloc(sdma->iram_pool, size, &buf_phys);
+		if (!buf_virt) {
+			use_iram = false;
+			buf_virt = dma_alloc_coherent(NULL, size, &buf_phys,
+					GFP_KERNEL);
+			if (!buf_virt)
+				return -ENOMEM;
+		}
+	} else {
+		buf_virt = sdma->tiny_datamem_buf;
+		buf_phys = sdma->tiny_datamem_buf_phys;
+	}
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	bd0->mode.command = C0_GETDM;
+	bd0->mode.status = BD_DONE | BD_INTR | BD_WRAP | BD_EXTD;
+	bd0->mode.count = size / 4;
+	bd0->buffer_addr = buf_phys;
+	bd0->ext_buffer_addr = address;
+
+	ret = sdma_run_channel0(sdma);
+
+	memcpy(buf, buf_virt, size);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	if (!tiny) {
+		if (use_iram)
+			gen_pool_free(sdma->iram_pool, (unsigned long)buf_virt,
+					size);
+		else
+			dma_free_coherent(NULL, size, buf_virt, buf_phys);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(sdma_fetch_datamem);
+
+int sdma_fetch_partial_context(struct sdma_channel *sdmac, void *buf,
+	u32 byte_offset, u32 num_bytes)
+{
+	static const u32 csz = sizeof(struct sdma_context_data);
+	u32 addr;
+	if (num_bytes > csz || num_bytes == 0 ||
+	byte_offset >= csz || byte_offset+num_bytes > csz ||
+	num_bytes % sizeof(u32) || byte_offset % sizeof(u32)) {
+		dev_err(sdmac->sdma->dev, "%s: invalid offset/length", __func__);
+		return -EINVAL;
+	}
+	addr = sdma_channel_context_base(sdmac->channel) + byte_offset/sizeof(u32);
+	return sdma_fetch_datamem(sdmac->sdma, buf, num_bytes, addr);
+}
+EXPORT_SYMBOL(sdma_fetch_partial_context);
 
-static void sdma_event_enable(struct sdma_channel *sdmac, unsigned int event)
+int sdma_event_enable(struct sdma_channel *sdmac, unsigned int event)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;
-	unsigned long val;
+	unsigned long oldval, val, flags;
 	u32 chnenbl = chnenbl_ofs(sdma, event);
 
-	val = readl_relaxed(sdma->regs + chnenbl);
+	spin_lock_irqsave(&sdma->event_enable_lock, flags);
+	oldval = val = readl_relaxed(sdma->regs + chnenbl);
 	__set_bit(channel, &val);
 	writel_relaxed(val, sdma->regs + chnenbl);
+	spin_unlock_irqrestore(&sdma->event_enable_lock, flags);
+	return (oldval & (1 << channel)) != 0;
+}
+EXPORT_SYMBOL(sdma_event_enable);
+
+unsigned long sdma_event_enable_by_channel_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask, unsigned int event)
+{
+	unsigned long oldval, val, flags;
+	u32 chnenbl = chnenbl_ofs(sdma, event);
+
+	spin_lock_irqsave(&sdma->event_enable_lock, flags);
+	oldval = val = readl_relaxed(sdma->regs + chnenbl);
+	val |= channel_mask;
+	writel_relaxed(val, sdma->regs + chnenbl);
+	spin_unlock_irqrestore(&sdma->event_enable_lock, flags);
+	return oldval;
 }
+EXPORT_SYMBOL(sdma_event_enable_by_channel_mask);
 
-static void sdma_event_disable(struct sdma_channel *sdmac, unsigned int event)
+int sdma_event_disable(struct sdma_channel *sdmac, unsigned int event)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;
 	u32 chnenbl = chnenbl_ofs(sdma, event);
-	unsigned long val;
+	unsigned long oldval, val, flags;
 
-	val = readl_relaxed(sdma->regs + chnenbl);
+	spin_lock_irqsave(&sdma->event_enable_lock, flags);
+	oldval = val = readl_relaxed(sdma->regs + chnenbl);
 	__clear_bit(channel, &val);
 	writel_relaxed(val, sdma->regs + chnenbl);
+	spin_unlock_irqrestore(&sdma->event_enable_lock, flags);
+	return (oldval & (1 << channel)) != 0;
+}
+EXPORT_SYMBOL(sdma_event_disable);
+
+unsigned long sdma_event_disable_by_channel_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask, unsigned int event)
+{
+	unsigned long oldval, val, flags;
+	u32 chnenbl = chnenbl_ofs(sdma, event);
+
+	spin_lock_irqsave(&sdma->event_enable_lock, flags);
+	oldval = val = readl_relaxed(sdma->regs + chnenbl);
+	val &= ~channel_mask;
+	writel_relaxed(val, sdma->regs + chnenbl);
+	spin_unlock_irqrestore(&sdma->event_enable_lock, flags);
+	return oldval;
 }
+EXPORT_SYMBOL(sdma_event_disable_by_channel_mask);
 
 static void sdma_update_channel_loop(struct sdma_channel *sdmac)
 {
@@ -844,10 +885,18 @@ static void mxc_sdma_handle_channel_normal(struct sdma_channel *data)
 		sdmac->status = DMA_COMPLETE;
 }
 
+static void sdma_custom_callback_tasklet(unsigned long data)
+{
+	struct sdma_channel *sdmac = (struct sdma_channel *) data;
+
+	if (sdmac->cb.callback)
+		sdmac->cb.callback(sdmac->cb.callback_param);
+}
+
 static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 {
 	struct sdma_engine *sdma = dev_id;
-	unsigned long stat;
+	unsigned long stat, flags;
 
 	stat = readl_relaxed(sdma->regs + SDMA_H_INTR);
 	writel_relaxed(stat, sdma->regs + SDMA_H_INTR);
@@ -859,9 +908,11 @@ static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 		struct sdma_channel *sdmac = &sdma->channel[channel];
 		struct sdma_desc *desc;
 
-		spin_lock(&sdmac->vc.lock);
+		spin_lock_irqsave(&sdmac->vc.lock, flags);
 		desc = sdmac->desc;
-		if (desc) {
+		if (sdmac->flags & IMX_DMA_CUSTOM_CALLBACK && sdmac->cb.callback) {
+			tasklet_schedule(&sdmac->cb_task);
+		} else if (desc) {
 			if (sdmac->flags & IMX_DMA_SG_LOOP) {
 				if (sdmac->peripheral_type != IMX_DMATYPE_HDMI)
 					sdma_update_channel_loop(sdmac);
@@ -876,7 +927,7 @@ static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 			}
 		}
 		__clear_bit(channel, &stat);
-		spin_unlock(&sdmac->vc.lock);
+		spin_unlock_irqrestore(&sdmac->vc.lock, flags);
 	}
 
 	return IRQ_HANDLED;
@@ -975,9 +1026,6 @@ static void sdma_get_pc(struct sdma_channel *sdmac,
 	case IMX_DMATYPE_HDMI:
 		emi_2_per = sdma->script_addrs->hdmi_dma_addr;
 		break;
-	case IMX_DMATYPE_MULTI_SAI:
-		per_2_emi = sdma->script_addrs->sai_2_mcu_addr;
-		emi_2_per = sdma->script_addrs->mcu_2_sai_addr;
 	default:
 		break;
 	}
@@ -1043,7 +1091,7 @@ static int sdma_load_context(struct sdma_channel *sdmac)
 	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
 	bd0->mode.count = sizeof(*context) / 4;
 	bd0->buffer_addr = sdma->context_phys;
-	bd0->ext_buffer_addr = 2048 + (sizeof(*context) / 4) * channel;
+	bd0->ext_buffer_addr = sdma_channel_context_base(channel);
 	ret = sdma_run_channel0(sdma);
 
 	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
@@ -1083,9 +1131,24 @@ static struct sdma_channel *to_sdma_chan(struct dma_chan *chan)
 	return container_of(chan, struct sdma_channel, vc.chan);
 }
 
-static int sdma_disable_channel(struct dma_chan *chan)
+int sdma_load_partial_context(struct sdma_channel *sdmac,
+	struct sdma_context_data *context, u32 byte_offset, u32 num_bytes)
+{
+	static const u32 csz = sizeof(*context);
+	u32 addr;
+	if (num_bytes > csz || num_bytes == 0 ||
+	byte_offset >= csz || byte_offset+num_bytes > csz ||
+	num_bytes % sizeof(u32) || byte_offset % sizeof(u32)) {
+		dev_err(sdmac->sdma->dev, "%s: invalid offset/length", __func__);
+		return -EINVAL;
+	}
+	addr = sdma_channel_context_base(sdmac->channel) + byte_offset/sizeof(u32);
+	return sdma_write_datamem(sdmac->sdma, context, num_bytes, addr);
+}
+EXPORT_SYMBOL(sdma_load_partial_context);
+
+static int sdma_disable_channel(struct sdma_channel *sdmac)
 {
-	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	struct sdma_engine *sdma = sdmac->sdma;
 	int channel = sdmac->channel;
 
@@ -1140,26 +1203,12 @@ static void sdma_set_watermarklevel_for_p2p(struct sdma_channel *sdmac)
 		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_DD;
 }
 
-static void sdma_set_watermarklevel_for_sais(struct sdma_channel *sdmac)
-{
-	sdmac->watermark_level &= ~(0xFFF << SDMA_WATERMARK_LEVEL_FIFOS_OFF |
-				    SDMA_WATERMARK_LEVEL_SW_DONE);
-
-	/* For fifo_num
-	 * bit 0-7 is the fifo number;
-	 * bit 8-11 is the fifo offset,
-	 * so here only need to shift left fifo_num 8 bit for watermake_level
-	 */
-	sdmac->watermark_level |= sdmac->fifo_num<<
-				SDMA_WATERMARK_LEVEL_FIFOS_OFF;
-}
-
 static int sdma_config_channel(struct dma_chan *chan)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	int ret;
 
-	sdma_disable_channel(chan);
+	sdma_disable_channel(sdmac);
 
 	sdmac->event_mask[0] = 0;
 	sdmac->event_mask[1] = 0;
@@ -1203,9 +1252,6 @@ static int sdma_config_channel(struct dma_chan *chan)
 			    sdmac->direction == DMA_MEM_TO_DEV &&
 			    sdmac->sdma->drvdata == &sdma_imx6ul)
 				__set_bit(31, &sdmac->watermark_level);
-			else if (sdmac->peripheral_type ==
-					IMX_DMATYPE_MULTI_SAI)
-				sdma_set_watermarklevel_for_sais(sdmac);
 
 			__set_bit(sdmac->event_id0, sdmac->event_mask);
 		}
@@ -1217,14 +1263,12 @@ static int sdma_config_channel(struct dma_chan *chan)
 		sdmac->watermark_level = 0; /* FIXME: M3_BASE_ADDRESS */
 	}
 
-	sdmac->context_loaded = false;
-
 	ret = sdma_load_context(sdmac);
 
 	return ret;
 }
 
-static int sdma_set_channel_priority(struct sdma_channel *sdmac,
+int sdma_set_channel_priority(struct sdma_channel *sdmac,
 		unsigned int priority)
 {
 	struct sdma_engine *sdma = sdmac->sdma;
@@ -1239,6 +1283,25 @@ static int sdma_set_channel_priority(struct sdma_channel *sdmac,
 
 	return 0;
 }
+EXPORT_SYMBOL(sdma_set_channel_priority);
+
+void sdma_set_channel_pending(struct sdma_channel *sdmac)
+{
+	struct sdma_engine *sdma = sdmac->sdma;
+	int channel = sdmac->channel;
+	unsigned long val = 0;
+
+	__set_bit(channel, &val);
+	writel_relaxed(val, sdma->regs + SDMA_H_EVTPEND);
+}
+EXPORT_SYMBOL(sdma_set_channel_pending);
+
+void sdma_set_channel_pending_by_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask)
+{
+	writel_relaxed(channel_mask, sdma->regs + SDMA_H_EVTPEND);
+}
+EXPORT_SYMBOL(sdma_set_channel_pending_by_mask);
 
 static int sdma_alloc_bd(struct sdma_desc *desc)
 {
@@ -1311,6 +1374,23 @@ static int sdma_request_channel0(struct sdma_engine *sdma)
 	return ret;
 }
 
+void sdma_set_channel_interrupt_callback(struct sdma_channel *sdmac,
+		dma_async_tx_callback int_cb, void *cb_param)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&sdmac->vc.lock, flags);
+	if (int_cb)
+		sdmac->flags |= IMX_DMA_CUSTOM_CALLBACK;
+	else
+		sdmac->flags &= ~IMX_DMA_CUSTOM_CALLBACK;
+	sdmac->cb.callback = int_cb;
+	sdmac->cb.callback_param = cb_param;
+	tasklet_init(&sdmac->cb_task, sdma_custom_callback_tasklet,
+					 (unsigned long) sdmac);
+	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
+}
+EXPORT_SYMBOL(sdma_set_channel_interrupt_callback);
+
 static struct sdma_desc *to_sdma_desc(struct dma_async_tx_descriptor *t)
 {
 	return container_of(t, struct sdma_desc, vd.tx);
@@ -1333,7 +1413,7 @@ static int sdma_channel_pause(struct dma_chan *chan)
 	if (!(sdmac->flags & IMX_DMA_SG_LOOP))
 		return -EINVAL;
 
-	sdma_disable_channel(chan);
+	sdma_disable_channel(sdmac);
 	spin_lock_irqsave(&sdmac->vc.lock, flags);
 	sdmac->status = DMA_PAUSED;
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
@@ -1389,7 +1469,7 @@ static int sdma_terminate_all(struct dma_chan *chan)
 		sdmac->desc = NULL;
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
 	vchan_dma_desc_free_list(&sdmac->vc, &head);
-	sdma_disable_channel(chan);
+	sdma_disable_channel(sdmac);
 	sdmac->context_loaded = false;
 
 	return 0;
@@ -1593,7 +1673,7 @@ static struct dma_async_tx_descriptor *sdma_prep_memcpy(
 			param &= ~BD_CONT;
 		}
 
-		dev_dbg(sdma->dev, "entry %d: count: %zd dma: 0x%x %s%s\n",
+		dev_dbg(sdma->dev, "entry %d: count: %d dma: 0x%u %s%s\n",
 				i, count, bd->buffer_addr,
 				param & BD_WRAP ? "wrap" : "",
 				param & BD_INTR ? " intr" : "");
@@ -1776,7 +1856,7 @@ static struct dma_async_tx_descriptor *sdma_prep_dma_cyclic(
 		if (i + 1 == num_periods)
 			param |= BD_WRAP;
 
-		dev_dbg(sdma->dev, "entry %d: count: %zd dma: %pad %s%s\n",
+		dev_dbg(sdma->dev, "entry %d: count: %d dma: %pad %s%s\n",
 				i, period_len, &dma_addr,
 				param & BD_WRAP ? "wrap" : "",
 				param & BD_INTR ? " intr" : "");
@@ -1801,14 +1881,11 @@ static int sdma_config(struct dma_chan *chan,
 		       struct dma_slave_config *dmaengine_cfg)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
-	/* clear watermark_level before setting */
-	sdmac->watermark_level = 0;
 	if (dmaengine_cfg->direction == DMA_DEV_TO_MEM) {
 		sdmac->per_address = dmaengine_cfg->src_addr;
 		sdmac->watermark_level = dmaengine_cfg->src_maxburst *
 			dmaengine_cfg->src_addr_width;
 		sdmac->word_size = dmaengine_cfg->src_addr_width;
-		sdmac->fifo_num =  dmaengine_cfg->src_fifo_num;
 	} else if (dmaengine_cfg->direction == DMA_DEV_TO_DEV) {
 		sdmac->per_address2 = dmaengine_cfg->src_addr;
 		sdmac->per_address = dmaengine_cfg->dst_addr;
@@ -1828,7 +1905,6 @@ static int sdma_config(struct dma_chan *chan,
 		sdmac->watermark_level = dmaengine_cfg->dst_maxburst *
 			dmaengine_cfg->dst_addr_width;
 		sdmac->word_size = dmaengine_cfg->dst_addr_width;
-		sdmac->fifo_num =  dmaengine_cfg->dst_fifo_num;
 	}
 	sdmac->direction = dmaengine_cfg->direction;
 	return sdma_config_channel(chan);
@@ -1925,7 +2001,7 @@ static void sdma_issue_pending(struct dma_chan *chan)
 #define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1	34
 #define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V2	38
 #define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V3	41
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	44
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	42
 
 static void sdma_add_scripts(struct sdma_engine *sdma,
 		const struct sdma_script_start_addrs *addr)
@@ -2111,6 +2187,20 @@ static int sdma_init(struct sdma_engine *sdma)
 		}
 	}
 
+#if TINY_DATAMEM_BUF_SIZE
+	sdma->tiny_datamem_buf = gen_pool_dma_alloc(sdma->iram_pool,
+			TINY_DATAMEM_BUF_SIZE, &sdma->tiny_datamem_buf_phys);
+	if (!sdma->tiny_datamem_buf) {
+		sdma->tiny_datamem_buf = dma_alloc_coherent(NULL,
+				TINY_DATAMEM_BUF_SIZE,
+				&sdma->tiny_datamem_buf_phys, GFP_KERNEL);
+		if (!sdma->tiny_datamem_buf) {
+			ret = -ENOMEM;
+			goto err_dma_alloc;
+		}
+	}
+#endif
+
 	sdma->context = (void *)sdma->channel_control +
 		MAX_DMA_CHANNELS * sizeof (struct sdma_channel_control);
 	sdma->context_phys = ccb_phys +
@@ -2139,10 +2229,7 @@ static int sdma_init(struct sdma_engine *sdma)
 
 	/* Set bits of CONFIG register but with static context switching */
 	/* FIXME: Check whether to set ACR bit depending on clock ratios */
-	if (sdma->clk_ratio)
-		writel_relaxed(SDMA_H_CONFIG_ACR, sdma->regs + SDMA_H_CONFIG);
-	else
-		writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);
+	writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);
 
 	writel_relaxed(ccb_phys, sdma->regs + SDMA_H_C0PTR);
 
@@ -2169,10 +2256,6 @@ static bool sdma_filter_fn(struct dma_chan *chan, void *fn_param)
 
 	if (!imx_dma_is_general_purpose(chan))
 		return false;
-	/* return false if it's not the right device */
-	if ((sdmac->sdma->drvdata == &sdma_imx8m)
-		&& (sdmac->sdma->idx != data->idx))
-		return false;
 
 	sdmac->data = *data;
 	chan->private = &sdmac->data;
@@ -2195,7 +2278,6 @@ static struct dma_chan *sdma_xlate(struct of_phandle_args *dma_spec,
 	data.dma_request = dma_spec->args[0];
 	data.peripheral_type = dma_spec->args[1];
 	data.priority = dma_spec->args[2];
-	data.idx = sdma->idx;
 
 	return dma_request_channel(mask, sdma_filter_fn, &data);
 }
@@ -2235,9 +2317,8 @@ static int sdma_probe(struct platform_device *pdev)
 	if (!sdma)
 		return -ENOMEM;
 
-	sdma->clk_ratio = of_property_read_bool(np, "fsl,ratio-1-1");
-
 	spin_lock_init(&sdma->channel_0_lock);
+	spin_lock_init(&sdma->event_enable_lock);
 
 	sdma->dev = &pdev->dev;
 	sdma->drvdata = drvdata;
@@ -2355,9 +2436,9 @@ static int sdma_probe(struct platform_device *pdev)
 	sdma->dma_device.device_terminate_all = sdma_terminate_all;
 	sdma->dma_device.device_pause = sdma_channel_pause;
 	sdma->dma_device.device_resume = sdma_channel_resume;
-	sdma->dma_device.src_addr_widths = SDMA_DMA_BUSWIDTHS;
-	sdma->dma_device.dst_addr_widths = SDMA_DMA_BUSWIDTHS;
-	sdma->dma_device.directions = SDMA_DMA_DIRECTIONS;
+	sdma->dma_device.src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);
+	sdma->dma_device.dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);
+	sdma->dma_device.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);
 	sdma->dma_device.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;
 	sdma->dma_device.device_prep_dma_memcpy = sdma_prep_memcpy;
 	sdma->dma_device.device_prep_dma_sg = sdma_prep_memcpy_sg;
@@ -2389,9 +2470,8 @@ static int sdma_probe(struct platform_device *pdev)
 		}
 		of_node_put(spba_bus);
 	}
-	/* There maybe multi sdma devices such as i.mx8mscale */
-	sdma->idx = sdma_dev_idx++;
 
+	sdma_singleton = sdma;
 	return 0;
 
 err_register:
@@ -2504,24 +2584,109 @@ static int sdma_resume(struct device *dev)
 	ret = sdma_get_firmware(sdma, sdma->fw_name);
 	if (ret) {
 		dev_warn(&pdev->dev, "failed to get firware\n");
-		goto out;
+		return ret;
 	}
 
 	ret = sdma_save_restore_context(sdma, false);
 	if (ret) {
 		dev_err(sdma->dev, "restore context error!\n");
-		goto out;
+		return ret;
 	}
 
-	ret = 0;
-out:
 	clk_disable(sdma->clk_ipg);
 	clk_disable(sdma->clk_ahb);
 
-	return ret;
+	return 0;
 }
 #endif
 
+struct sdma_channel *sdma_get_channel(struct sdma_engine *sdma, int channel)
+{
+	if (channel < 0 || channel >= MAX_DMA_CHANNELS) {
+		return NULL;
+	}
+	return &sdma->channel[channel];
+}
+EXPORT_SYMBOL(sdma_get_channel);
+
+/* (mjs) convenience function for initializing a channel as
+ * host-triggered or event-triggered
+ * external=false: channel started by host, HO[i]=0, EO[i]=1
+ * external=true: channel started by event, HO[i]=1, EO[i]=0 */
+void sdma_setup_channel(struct sdma_channel *sdmac, bool external)
+{
+	sdma_disable_channel(sdmac);
+	sdma_config_ownership(sdmac,
+		external,   /* event override */
+		!external,  /* host override */
+		false);     /* always false */
+}
+EXPORT_SYMBOL(sdma_setup_channel);
+
+struct sdma_engine *sdma_engine_get(void)
+{
+	return sdma_singleton;
+}
+EXPORT_SYMBOL(sdma_engine_get);
+
+ssize_t sdma_print_context(struct sdma_engine *sdma, int channel, char *buf)
+{
+	static const char *regnames[] = {
+		" r0", " r1", " r2", " r3", " r4", " r5", " r6", " r7",
+		"mda", "msa", " ms", " md", "pda", "psa", " ps", " pd",
+		" ca", " cs", "dda", "dsa", " ds", " dd", "sc0", "sc1",
+		"sc2", "sc3", "sc4", "sc5", "sc6", "sc7"
+	};
+
+	struct sdma_context_data *context;
+	u32 context_addr = sdma_channel_context_base(channel);
+	u32 context_size = sizeof(*context);
+	u32 *regptr;
+	int ret;
+	int i;
+	ssize_t outlen = 0;
+
+	context = kzalloc(context_size, GFP_ATOMIC);
+	if (!context) {
+		return -ENOMEM;
+	}
+
+	ret = sdma_fetch_datamem(sdma, context, context_size, context_addr);
+	if (ret) {
+		return ret;
+	}
+
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+		"pc=%04x rpc=%04x spc=%04x epc=%04x\n",
+		context->channel_state.pc,
+		context->channel_state.rpc,
+		context->channel_state.spc,
+		context->channel_state.epc
+	);
+
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+		"Flags: t=%d sf=%d df=%d lm=%d\n",
+		(context->channel_state.t != 0),
+		(context->channel_state.sf != 0),
+		(context->channel_state.df != 0),
+		(context->channel_state.lm != 0)
+	);
+
+	regptr = &context->gReg[0];
+	for (i = 0; i < ARRAY_SIZE(regnames); i++) {
+		outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen,
+			"%s=%08x%c",
+			regnames[i],
+			regptr[i],
+			((i % 6) == 5) ? '\n' : ' ');
+	}
+	outlen += scnprintf(buf+outlen, PAGE_SIZE-outlen, "\n");
+
+	kfree(context);
+	return outlen;
+}
+EXPORT_SYMBOL(sdma_print_context);
+
 static const struct dev_pm_ops sdma_pm_ops = {
 	SET_LATE_SYSTEM_SLEEP_PM_OPS(sdma_suspend, sdma_resume)
 };
diff --git a/include/linux/platform_data/dma-imx-sdma.h b/include/linux/platform_data/dma-imx-sdma.h
index 3ecbf2fac02c..c6b87f57185a 100644
--- a/include/linux/platform_data/dma-imx-sdma.h
+++ b/include/linux/platform_data/dma-imx-sdma.h
@@ -1,6 +1,8 @@
 #ifndef __MACH_MXC_SDMA_H__
 #define __MACH_MXC_SDMA_H__
 
+#include <linux/dmaengine.h>
+
 /**
  * struct sdma_script_start_addrs - SDMA script start pointers
  *
@@ -52,10 +54,6 @@ struct sdma_script_start_addrs {
 	s32 zqspi_2_mcu_addr;
 	s32 mcu_2_ecspi_addr;
 	/* End of v3 array */
-	s32 mcu_2_zqspi_addr;
-	s32 mcu_2_sai_addr;
-	s32 sai_2_mcu_addr;
-	/* End of v4 array */
 };
 
 /**
@@ -69,4 +67,326 @@ struct sdma_platform_data {
 	struct sdma_script_start_addrs *script_addrs;
 };
 
+/*
+ * Mode/Count of data node descriptors - IPCv2
+ */
+struct sdma_mode_count {
+	u32 count   : 16; /* size of the buffer pointed by this BD */
+	u32 status  :  8; /* E,R,I,C,W,D status bits stored here */
+	u32 command :  8; /* command mostlky used for channel 0 */
+};
+
+/*
+ * Buffer descriptor
+ */
+struct sdma_buffer_descriptor {
+	struct sdma_mode_count  mode;
+	u32 buffer_addr;	/* address of the buffer described */
+	u32 ext_buffer_addr;	/* extended buffer address */
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_channel_control - Channel control Block
+ *
+ * @current_bd_ptr	current buffer descriptor processed
+ * @base_bd_ptr		first element of buffer descriptor array
+ * @unused		padding. The SDMA engine expects an array of 128 byte
+ *			control blocks
+ */
+struct sdma_channel_control {
+	u32 current_bd_ptr;
+	u32 base_bd_ptr;
+	u32 unused[2];
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_state_registers - SDMA context for a channel
+ *
+ * @pc:		program counter
+ * @t:		test bit: status of arithmetic & test instruction
+ * @rpc:	return program counter
+ * @sf:		source fault while loading data
+ * @spc:	loop start program counter
+ * @df:		destination fault while storing data
+ * @epc:	loop end program counter
+ * @lm:		loop mode
+ */
+struct sdma_state_registers {
+	u32 pc     :14;
+	u32 unused1: 1;
+	u32 t      : 1;
+	u32 rpc    :14;
+	u32 unused0: 1;
+	u32 sf     : 1;
+	u32 spc    :14;
+	u32 unused2: 1;
+	u32 df     : 1;
+	u32 epc    :14;
+	u32 lm     : 2;
+} __attribute__ ((packed));
+
+/**
+ * struct sdma_context_data - sdma context specific to a channel
+ *
+ * @channel_state:	channel state bits
+ * @gReg:		general registers
+ * @mda:		burst dma destination address register
+ * @msa:		burst dma source address register
+ * @ms:			burst dma status register
+ * @md:			burst dma data register
+ * @pda:		peripheral dma destination address register
+ * @psa:		peripheral dma source address register
+ * @ps:			peripheral dma status register
+ * @pd:			peripheral dma data register
+ * @ca:			CRC polynomial register
+ * @cs:			CRC accumulator register
+ * @dda:		dedicated core destination address register
+ * @dsa:		dedicated core source address register
+ * @ds:			dedicated core status register
+ * @dd:			dedicated core data register
+ */
+struct sdma_context_data {
+	struct sdma_state_registers  channel_state;
+	u32  gReg[8];
+	u32  mda;
+	u32  msa;
+	u32  ms;
+	u32  md;
+	u32  pda;
+	u32  psa;
+	u32  ps;
+	u32  pd;
+	u32  ca;
+	u32  cs;
+	u32  dda;
+	u32  dsa;
+	u32  ds;
+	u32  dd;
+	u32  scratch0;
+	u32  scratch1;
+	u32  scratch2;
+	u32  scratch3;
+	u32  scratch4;
+	u32  scratch5;
+	u32  scratch6;
+	u32  scratch7;
+} __attribute__ ((packed));
+
+#define NUM_BD (int)(PAGE_SIZE / sizeof(struct sdma_buffer_descriptor))
+#define SDMA_BD_MAX_CNT	0xfffc /* align with 4 bytes */
+
+struct sdma_engine;
+
+struct sdma_channel;
+
+/**
+ * sdma_engine_get() - returns a pointer to the global SDMA engine
+ *
+ * Return: pointer to the sdma_engine object
+ */
+struct sdma_engine *sdma_engine_get(void);
+
+/**
+ * sdma_get_channel() - returns a pointer to the numbered SDMA channel
+ * @sdma:	pointer to the sdma_engine object
+ * @channel:	channel number from 0-31
+ *
+ * Return: pointer to channel object, or NULL
+ */
+struct sdma_channel *sdma_get_channel(struct sdma_engine *sdma, int channel);
+
+/**
+ * sdma_set_channel_interrupt_callback() - sets a custom interrupt handler
+ * @sdmac:	pointer to sdma_channel object
+ * @int_cb:	callback function to register
+ * @cb_param:	user-defined object passed when the callback is invoked
+ *
+ * Sets a function to be called when a custom SDMA script triggers an interrupt
+ * (e.g. with a "done 3" instruction).
+ * The function is executed in tasklet (atomic) context.
+ */
+void sdma_set_channel_interrupt_callback(struct sdma_channel *sdmac,
+		dma_async_tx_callback int_cb, void *cb_param);
+
+/**
+ * sdma_set_channel_priority() - sets the channel's execution priority
+ * @sdmac:	pointer to sdma_channel object
+ * @priority:	priority, from 0 (disabled) to 7 (highest)
+ *
+ * Setting a nonzero priority may cause the channel's script to begin executing,
+ * depending on how it is configured.
+ * Priority 7 is used by channel 0 for loading scripts/context. Typically,
+ * channel 0 should be the only channel with priority 7.
+ *
+ * Return: 0 on success, nonzero otherwise
+ */
+int sdma_set_channel_priority(struct sdma_channel *sdmac,
+		unsigned int priority);
+
+/**
+ * sdma_set_channel_pending() - sets the channel as pending
+ * @sdmac:	pointer to sdma_channel object
+ */
+void sdma_set_channel_pending(struct sdma_channel *sdmac);
+
+/**
+ * sdma_set_channel_pending_by_channel_mask() - sets a group of channels as
+ * pending
+ * @sdma:	pointer to the sdma_engine object
+ * @channel_mask: bitmask of channels to set pending
+ */
+void sdma_set_channel_pending_by_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask);
+
+/**
+ * sdma_setup_channel() - convenience function for setting channel ownership
+ * @sdmac:	pointer to sdma_channel object
+ * @external:	if true, script is triggered by an external event,
+ *		if false, script is triggered by the CPU
+ */
+void sdma_setup_channel(struct sdma_channel *sdmac, bool external);
+
+/**
+ * sdma_event_enable() - allows a channel to be triggered by the numbered event
+ * @sdmac:	pointer to sdma_channel object
+ * @event:	event number (see reference manual)
+ * Return:	0 if the event was previously disabled,
+ *          nonzero if previously enabled
+ */
+int sdma_event_enable(struct sdma_channel *sdmac, unsigned int event);
+
+/**
+ * sdma_event_enable_by_channel_mask() - allows a group of channels to be
+ * triggered by the numbered event
+ * @sdma:	pointer to the sdma_engine object
+ * @channel_mask: bitmask of channels to enable
+ * @event:	event number (see reference manual)
+ * Return:	previous channel enable bitmask for the event
+ */
+unsigned long sdma_event_enable_by_channel_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask, unsigned int event);
+
+/**
+ * sdma_event_disable() - prevents a channel from being triggered by an event
+ * @sdmac:	pointer to sdma_channel object
+ * @event:	event number (see reference manual)
+ * Return:	0 if the event was previously disabled,
+ *          nonzero if previously enabled
+ */
+int sdma_event_disable(struct sdma_channel *sdmac, unsigned int event);
+
+/**
+ * sdma_event_disable_by_channel_mask() - prevents a group of channels from
+ * being triggered by an event
+ * @sdma:	pointer to the sdma_engine object
+ * @channel_mask: bitmask of channels to disable
+ * @event:	event number (see reference manual)
+ * Return:	previous channel enable bitmask for the event
+ */
+unsigned long sdma_event_disable_by_channel_mask(struct sdma_engine *sdma,
+	unsigned int channel_mask, unsigned int event);
+
+/**
+ * sdma_is_event_enabled() - returns whether an event can trigger the channel
+ * @sdmac:	pointer to sdma_channel object
+ * @event:	event number (see reference manual)
+ * Return: 0 if disabled, nonzero if enabled
+ */
+int sdma_is_event_enabled(struct sdma_channel *sdmac, unsigned int event);
+
+/* address should be in program space (halfword addressing) */
+
+/**
+ * sdma_load_script() - copies script from ARM memory to SDMA memory
+ * @sdma:	pointer to sdma_engine object
+ * @buf:	start of script
+ * @size:	size of script in bytes
+ * @address:	destination address in SDMA program space
+ *		(using halfword addressing)
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
+		u32 address);
+
+/**
+ * sdma_load_partial_context() - writes a subset of a channel's context
+ * @sdmac:		pointer to sdma_channel object
+ * @context:		pointer to data to write
+ * @byte_offset:	destination offset within the channel's context RAM
+ *			(must be a multiple of 4 and less than 128)
+ * @num_bytes:		number of bytes to copy into the channel's context RAM
+ *			(must be > 0 and <= 128)
+ *
+ * Can be used to update a subset of a channel's registers while leaving others
+ * undisturbed, e.g. to change a script's arguments while it is running without
+ * overwriting internal state.
+ * Since RAM loading is handled by channel 0, and channels cannot preempt each
+ * other, the load operation is mutually exclusive with the channel's execution.
+ * (i.e. a channel's registers will not change while its script is executing.)
+ *
+ * Example: to update a channel's entire context, use byte_offset=0 and
+ * num_bytes=128.
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_load_partial_context(struct sdma_channel *sdmac,
+	struct sdma_context_data *context,
+	u32 byte_offset,
+	u32 num_bytes);
+
+/* size should be a value in bytes */
+/* address should be in data space (word addressing) */
+
+/**
+ * sdma_write_datamem() - writes data into the SDMA engine's address space
+ * @sdma:	pointer to sdma_engine object
+ * @buf:	data to write
+ * @size:	number of bytes to write
+ * @address:	destination offset, in 32-bit words, from the origin of SDMA
+ *		address space
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_write_datamem(struct sdma_engine *sdma, void *buf, int size,
+	u32 address);
+
+/**
+ * sdma_fetch_partial_context() - reads a subset of a channel's context
+ * @sdmac:		pointer to sdma_channel object
+ * @buf:		buffer to receive data
+ * @byte_offset:	source offset within the channel's context RAM
+ *			(must be a multiple of 4 and less than 128)
+ * @num_bytes:		number of bytes to read from the channel's context RAM
+ *			(must be > 0 and <= 128)
+ *
+ * Since RAM loading is handled by channel 0, and channels cannot preempt each
+ * other, the fetch operation is mutually exclusive with the channel's
+ * execution. (i.e. the values will not be changing at the same time as they are
+ * being read.)
+ *
+ * Example: to fetch a channel's entire context, use byte_offset=0 and
+ * num_bytes=128.
+ *
+ * buf must be large enough to hold num_bytes of data.
+ *
+ * Return: 0 on success, nonzero on error
+ */
+int sdma_fetch_partial_context(struct sdma_channel *sdmac, void *buf,
+    u32 byte_offset,
+    u32 num_bytes);
+
+/**
+ * sdma_print_context() - dump string representation of channel context values
+ * @sdma:	pointer to sdma_engine object
+ * @channel:	channel number, 0-31
+ * @buf:	buffer to receive the string
+ *
+ * Prints a string representation of all channel registers and scratch memory
+ * words. buf should be at least 512 bytes long. Useful for debugging.
+ *
+ * Return: result string length in bytes, or < 0 on error
+ */
+ssize_t sdma_print_context(struct sdma_engine *sdma, int channel, char *buf);
+
 #endif /* __MACH_MXC_SDMA_H__ */
-- 
2.17.1

